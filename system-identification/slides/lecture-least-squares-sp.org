#+OPTIONS: toc:nil
# #+LaTeX_CLASS: koma-article 

#+LATEX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [presentation,aspectratio=169]
#+OPTIONS: H:2

#+LaTex_HEADER: \usepackage{khpreamble}
#+LaTex_HEADER: \usepackage{amssymb}
#+LaTex_HEADER: \usepackage{pgfplotstable}
#+LaTex_HEADER: \DeclareMathOperator{\shift}{q}
#+LaTex_HEADER: \DeclareMathOperator{\diff}{p}

#+title: Control computarizado - Mínimos cuadrados
# #+date: 2018-10-03

* What do I want the students to understand?			   :noexport:
  - Least squares

* Which activities will the students do?			   :noexport:
  1. Determine order of the controller
  2. Set up equations in controller parameters

* Intro
** Identificación de sistemas

** Un proceso complejo

*** Graphics 
   :PROPERTIES:
    :BEAMER_col: 0.6
    :END:
 From Wikipedia "Cyclonic separation"
*** Graphics 
   :PROPERTIES:
    :BEAMER_col: 0.4
    :END:
   #+begin_center
   \includegraphics[height=1.0\textheight]{../../figures/Vertical-cyclone.jpg}
   #+end_center

** Identificación de sistemas
   #+begin_export latex
   \begin{center}
     \begin{tikzpicture}[node distance=22mm, block/.style={rectangle, draw, minimum width=15mm, inner sep=10pt}, sumnode/.style={circle, draw, inner sep=2pt},]
    
       \node[coordinate] (input) {};
       \node[coordinate, right of=input] (copy) {};
       \node[coordinate, right of=copy] (midp) {};
       \node[block, above of=midp, node distance=10mm] (sys)  {Sistema};
       \node[block, below of=midp, node distance=10mm] (mod)  {Modelo};
       \node[sumnode, right of=midp, node distance=26mm] (sum) {\tiny $\Sigma$};
       \node[coordinate, right of=sum, node distance=22mm] (output) {};

       \draw[-] (input) -- node[above, pos=0.2] {Señal medida} (copy);
       \draw[->] (copy) |- node[above] {} (sys);
       \draw[->] (copy) |- node[above] {} (mod);
       \draw[->] (sys) -| node[left, pos=0.9] {$+$} (sum);
       \draw[->] (mod) -| node[left, pos=0.9] {$-$} (sum);
       \draw[->] (sum) -- node[above, near end] {Error} (output);

       \draw[thick, red!70!black, ->] (2.7,-2) -- (3.3,-2) -- (5.3, 0);
     \end{tikzpicture}
   \end{center}

   #+end_export

** Ajustando un modelo
   #+begin_center
   \includegraphics[height=0.8\textheight]{lsq-example-no-reg}
   #+end_center

** Ajustando un modelo - regresión lineal

*** Text
    :PROPERTIES:
    :BEAMER_col: 0.4
    :END:

    *Objetivo* Dado observaciones \[\mathcal{D} = \{ (x_1,y_1), (x_2, y_2), \ldots, (x_N, y_N)\}\] y 
    modelo \( \mathcal{M}: \; y = ax + b  + e\), obtiene los parametros \( (a,b)\) que da el modelo que mejor se ajuste a los datos.

    El término de ruido, o error, \(e\), incluye errores de modelación y perturbaciones.
*** Graphics
    :PROPERTIES:
    :BEAMER_col: 0.6
    :END:
   #+begin_center
   \includegraphics[height=0.6\textheight]{lsq-example}
   #+end_center


** Ajustando un modelo - regresión lineal

    Dado observaciones \(\mathcal{D} = \{ (x_1,y_1), (x_2, y_2), \ldots, (x_N, y_N)\}\) y 
    modelo \( \mathcal{M}: \; y = ax + b  + e\). 

*** Text
    :PROPERTIES:
    :BEAMER_col: 0.7
    :END:

    La predicción es
    \[ \hat{y_k} = ax_k + b = \underbrace{\begin{bmatrix} x & 1 \end{bmatrix}}_{\varphi_k^T} \underbrace{\begin{bmatrix} a\\b\end{bmatrix}}_{\theta}\]
    y el error de predicción 
    \[ \epsilon_k = y_k - \hat{y}_k = y_k - ax_k-b = y - \varphi_k^T\theta.\]

    Buscamos parametros \(\theta^T = \begin{bmatrix} a & b \end{bmatrix}\) que minimiza
     la función de pérdida \[J(\theta) =  \sum_{k=1}^N g\big(\epsilon_k\big).\]
    
*** Graphics
    :PROPERTIES:
    :BEAMER_col: 0.3
    :END:
   #+begin_center
   \includegraphics[height=0.4\textheight]{lsq-example}
   #+end_center


** Ajustando un modelo - regresión lineal

    Dado observaciones \(\mathcal{D} = \{ (x_1,y_1), (x_2, y_2), \ldots, (x_N, y_N)\}\) y 
    modelo \( \mathcal{M}: \; y = ax + b  + e\). 

*** Text
    :PROPERTIES:
    :BEAMER_col: 0.6
    :END:
    
    La función de pérdida más común es *mínimos cuadrados*

    \begin{align*}
    \hat{\theta}_{LS} &= \arg\min J_{LS}(\theta) = \arg\min \sum_{k=1}^N \epsilon_k^2\\
    &= \arg\min \sum_{k=1}^N (y_k - \hat{y}_k)^2 
    = \arg\min \sum_{k=1}^N (y_k - \varphi_k\T\theta)^2\\ 
    &= \arg\min \sum_{k=1}^N (y_k - ax_k - b)^2
    \end{align*}
    
*** Graphics
    :PROPERTIES:
    :BEAMER_col: 0.4
    :END:
   #+begin_center
   \includegraphics[height=0.5\textheight]{lsq-example}
   #+end_center



** El problema con mínimos cuadrados
*** Text
    :PROPERTIES:
    :BEAMER_col: 0.4
    :END:
   \begin{align*}
    \text{minimiza} \; &\sum_k g(\epsilon_k)\\
    \text{dónde} \; g(u) &= u^2
   \end{align*}
   
*** Graphics
    :PROPERTIES:
    :BEAMER_col: 0.6
    :END:

    #+begin_export latex
        \begin{center}
          \begin{tikzpicture}
            \begin{axis}[
              width=8cm,
              height=6cm,
              ylabel=pérdida,
              xlabel=residual,
              ]
              \addplot[red, thick, no marks, domain=-4:4, samples=201] {x^2};
            \end{axis}
          \end{tikzpicture}
        \end{center}

    #+end_export

** Más robusta: La función de pérdida de Huber
*** Text
    :PROPERTIES:
    :BEAMER_col: 0.4
    :END:
    También conocido como *regresión robusta*
   \begin{align*}
    \text{minimiza} \; &\sum_k g_{hub}(\epsilon_k)\\
    \text{dónde}\; g_{hub}(u) &= \begin{cases} u^2 & |u| \le M\\ M(2|u|-M) & |u| > M \end{cases}
   \end{align*}

*** Graphics
    :PROPERTIES:
    :BEAMER_col: 0.6
    :END:
    #+begin_export latex
        \begin{center}
          \begin{tikzpicture}
            \begin{axis}[
              width=8cm,
              height=6cm,
              ylabel=penalty,
              xlabel=residual,
              ]
              \addplot[red, thick, no marks, domain=-4:4, samples=201] {x^2};
              \addplot[orange!90!black, ultra thick, no marks, domain=-4:-1, samples=201] {2*abs(x)-1};
              \addplot[orange!90!black, thin, no marks, domain=-1:1, samples=201] {x^2};
              \addplot[orange!90!black, ultra thick, no marks, domain=1:4, samples=201] {2*abs(x)-1};
            \end{axis}
          \end{tikzpicture}
        \end{center}

    #+end_export

* AR-model

** Ejemplo - Modelo autorregresivo (AR)
** Modelo autorregresivo (AR)
Dado una secuencia discreta observada \(y(k), \; k=1,2,\ldots,N\), y el modelo autorregresivo
\[ y(k+1) = -ay(k) + e(k+1),\]
dónde \(e(k)\) es una sequencia discreto de ruido blanco.

*Objetivo* Estimar el parametro \(a\).

1. Forma el predictor de un paso adelante \[\hat{y}_{k+1} = -ay_k=-y_ka = \varphi_{k+1} \theta,\] y el error de predicción \[\epsilon_k = y_k - \hat{y}_k = y_k - \varphi_k \theta\]


** Modelo autorregresivo (AR)
Dado una secuencia discreta observada \(y(k), \; k=1,2,\ldots,N\), y el modelo autorregresivo
\( y(k+1) = -ay(k) + e(k+1),\)
dónde \(e(k)\) es una sequencia discreto de ruido blanco.

*Objetivo* Estimar el parametro \(a\).

2. [@2] Reune todas las observaciónes \(y_k\) y predicciones \(\hat{y}_k\) en forma vectoral
   \begin{align*}
   \epsilon &= \begin{bmatrix} \epsilon_2\\\epsilon_2\\\vdots\\\epsilon_N\end{bmatrix} =  \begin{bmatrix} y_2\\ y_3\\\vdots\\y_N \end{bmatrix} - \begin{bmatrix} \hat{y}_2\\ \hat{y}_3\\\vdots\\\hat{y}_N \end{bmatrix}
    =  \begin{bmatrix} y_2\\ y_3\\\vdots\\y_N \end{bmatrix} - \begin{bmatrix} -y_1 a\\ -y_2 a\\\vdots\\-y_{N-1}^T\theta \end{bmatrix} =  \begin{bmatrix} y_2\\ y_3\\\vdots\\y_N \end{bmatrix} - \begin{bmatrix} \varphi_2^T\theta\\ \varphi_3^T\theta\\\vdots\\\varphi_N^T\theta \end{bmatrix}\\
   &= y - \underbrace{\begin{bmatrix}\varphi_1^T\\\varphi_2^T\\\vdots\\\varphi_N^T\end{bmatrix}}_{\Phi}\theta = y - \Phi\theta 
   \end{align*}



** Modelo autorregresivo (AR)
Dado una secuencia discreta observada \(y(k), \; k=1,2,\ldots,N\), y el modelo autorregresivo
\( y(k+1) = -ay(k) + e(k+1),\)
dónde \(e(k)\) es una sequencia discreto de ruido blanco.

*Objetivo* Estimar el parametro \(a\).

3. [@3] Obtiene el estimado de mínimos cuadrados 
   \begin{align*}
    \theta_{LS} &= (\Phi^T\Phi)^{-1}\Phi^T y\\ &= \left(\begin{bmatrix} -y_1 & -y_2 & \cdots & -y_{N-1}\end{bmatrix}\begin{bmatrix}-y_1\\-y_2\\\vdots\\-y_{N-1}\end{bmatrix}\right)^{-1}\begin{bmatrix} -y_1 & -y_2 & \cdots & -y_{N-1}\end{bmatrix}\begin{bmatrix}y_2\\y_3\\\vdots\\y_N\end{bmatrix}\\
    &= -\frac{\sum_{k=1}^{N-1} y_ky_{k+1}}{\sum_{k=1}^{N-1}y_k^2}
    \end{align*}


** Computación de la solución de mínimos cuadrados
   Dado error de predicción en forma vectoral para sistema de orden $n$
   \( \epsilon = y - \Phi\theta\). Forma las *ecuaciones normales*
   \begin{align*}
   \Phi \theta &= y\\
   \begin{bmatrix}\varphi_{n+1}^T\\\varphi_{n+2}^T\\\varphi_{n+3}^T\\\varphi_{n+4}^T\\\vdots\\\varphi_{N}^T\end{bmatrix} \begin{bmatrix}\theta_1\\\theta_2\\\vdots\\\theta_m\end{bmatrix} &= \begin{bmatrix}y_{n+1}\\y_{n+2}\\y{n+3}\\y_{n+4}\\\vdots\\ y_{N}\end{bmatrix}
   \end{align*}
   Resuelva las ecuaciones normales usando métodos numericamente robustos de algebra lineal, por ejemplo   factorización L-U. En matlab se escribe
   #+begin_src octave
	theta_LS = Phi \ y
   #+end_src
   
** Ejemplo numerico 
  
   [[https://mybinder.org/v2/gh/kjartan-at-tec/mr2007-computerized-control/master?filepath=system-identification%2Fnotebooks%2FAR-example.ipynb][Mybinder]]



   

** Modelo autorregresivo (AR) - Ejercicio
Dado una secuencia discreta observada \(y(k), \; k=1,2,\ldots,N\), y el modelo autorregresivo de segunda orden
\[ y(k+2) = a_1y(k+1) + a_2y(k) + e(k+2),\]
dónde \(e(k)\) es una sequencia discreto de ruido blanco.

*Actividad* Forma las ecuaciones normales \[ \Phi \theta = y\] siguiendo los mismos pasos como en el ejemplo de primer orden.


