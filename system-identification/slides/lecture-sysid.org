#+OPTIONS: toc:nil
# #+LaTeX_CLASS: koma-article 

#+LATEX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [presentation,aspectratio=169]
#+OPTIONS: H:2

#+LaTex_HEADER: \usepackage{khpreamble}
#+LaTex_HEADER: \usepackage{amssymb}
#+LaTex_HEADER: \usepackage{pgfplotstable}
#+LaTex_HEADER: \DeclareMathOperator{\shift}{q}
#+LaTex_HEADER: \DeclareMathOperator{\diff}{p}

#+title: System identification
# #+date: 2018-10-03

* What do I want the students to understand?			   :noexport:
  - Least squares parameter estimation

* Which activities will the students do?			   :noexport:
  1. Determine order of the controller
  2. Set up equations in controller parameters

* Intro
** A complicated process

*** Graphics 
   :PROPERTIES:
    :BEAMER_col: 0.6
    :END:
 From Wikipedia "Cyclonic separation"
*** Graphics 
   :PROPERTIES:
    :BEAMER_col: 0.4
    :END:
   #+begin_center
   \includegraphics[height=1.0\textheight]{../../figures/Vertical-cyclone.jpg}
   #+end_center

** System identification
   #+begin_export latex
   \begin{center}
     \begin{tikzpicture}[node distance=22mm, block/.style={rectangle, draw, minimum width=15mm, inner sep=10pt}, sumnode/.style={circle, draw, inner sep=2pt},]
    
       \node[coordinate] (input) {};
       \node[coordinate, right of=input] (copy) {};
       \node[coordinate, right of=copy] (midp) {};
       \node[block, above of=midp, node distance=10mm] (sys)  {System};
       \node[block, below of=midp, node distance=10mm] (mod)  {Model};
       \node[sumnode, right of=midp, node distance=26mm] (sum) {\tiny $\Sigma$};
       \node[coordinate, right of=sum, node distance=22mm] (output) {};

       \draw[-] (input) -- node[above, pos=0.2] {Measured input} (copy);
       \draw[->] (copy) |- node[above] {} (sys);
       \draw[->] (copy) |- node[above] {} (mod);
       \draw[->] (sys) -| node[left, pos=0.9] {$+$} (sum);
       \draw[->] (mod) -| node[left, pos=0.9] {$-$} (sum);
       \draw[->] (sum) -- node[above, near end] {Error} (output);

       \draw[thick, red!70!black, ->] (2.7,-2) -- (3.3,-2) -- (5.3, 0);
     \end{tikzpicture}
   \end{center}

   #+end_export

** The Auto-Regressive with eXogenous input (ARX) model 

#+begin_center
\includegraphics[width=0.7\linewidth]{../../figures/block-arx}
#+end_center
 \[ A(\shift) y(k) = B(\shift)u(k) + e(k+n) \]

    The error signal \(e(k)\) is a zero-mean white noise sequence representing perturbations and modeling errors.

** First-order ARX model with one delay

#+begin_center
\includegraphics[width=0.3\linewidth]{../../figures/block-arx}
#+end_center
 \[ (\shift + a_1) y(k) = (b_0 \shift + b_1) \shift^{-1}u(k) + e(k+1) \]
 \[ y(k+1) +  a_1y(k) = b_0u(k) + b_1u(k-1) + e(k+1) \]

 #+BEAMER: \pause
 
Using the model to predict the output one step ahead:
\begin{align*}
 \hat{y}(k+1) &= -a_1y(k) + b_0u(k) + b_1u(k-1) =  \underbrace{\begin{bmatrix} -y(k) & u(k) & u(k-1) \end{bmatrix}}_{\varphi_{k+1}^T} \underbrace{\begin{bmatrix} a_1\\b_0\\b_1\end{bmatrix}}_{\theta}\\
 &= \varphi_{k+1}^T\theta
 \end{align*}

 

** Parameter estimation - Least squares

*Objective* Given observations \[\mathcal{D} = \{ (u_1,y_1), (u_2, y_2), \ldots, (u_N, y_N)\}\] and model \( \mathcal{M}: \; y(k+1) = -ay(k) + b_0u(k) + b_1u(k-1)  + e(k+1)\), obtain the parameters \( (a_1,\,b_0,\,b_1)\) which gives the best fit of the model to the data.

 

** Parameter estimation - Least squares
Given observations \[\mathcal{D} = \{ (u_1,y_1), (u_2, y_2), \ldots, (u_N, y_N)\}\] and model \( \mathcal{M}: \; y(k+1) = -ay(k) + b_0u(k) + b_1u(k-1)  + e(k+1)\).

1. Form the one-step ahead prediction
 \[ \hat{y}_{k+1} = -a_1y_k + b_0u_k + b_1u_{k-1} =  \underbrace{\begin{bmatrix} -y_k & u_k & u_{k-1} \end{bmatrix}}_{\varphi_{k+1}^T} \underbrace{\begin{bmatrix} a_1\\b_0\\b_1\end{bmatrix}}_{\theta}\] and the prediction error
    \[ \epsilon_{k+1} = y_{k+1} - \hat{y}_{k+1} = y_{k+1} - \varphi_{k+1}^T\theta.\]


** Parameter estimation - Least squares

2. [@2] Combine all the observations \(y_k\) and predictions \(\hat{y}_k\) on vector form
   \begin{align*}
   \epsilon &= \begin{bmatrix} \epsilon_3\\\epsilon_4\\\vdots\\\epsilon_N\end{bmatrix} =  \begin{bmatrix} y_3\\ y_4\\\vdots\\y_N \end{bmatrix} - \begin{bmatrix} \hat{y}_3\\ \hat{y}_4\\\vdots\\\hat{y}_N \end{bmatrix}
    =  \begin{bmatrix} y_2\\ y_3\\\vdots\\y_N \end{bmatrix} - \begin{bmatrix} \varphi_2^T\theta\\ \varphi_3^T\theta\\\vdots\\\varphi_N^T\theta \end{bmatrix}\\
   &= y - \underbrace{\begin{bmatrix}\varphi_1^T\\\varphi_2^T\\\vdots\\\varphi_N^T\end{bmatrix}}_{\Phi}\theta = y - \Phi\theta 
   \end{align*}
3. Solve \(\arg\min \; J(\theta) = \frac{1}{2}\epsilon^T\epsilon = \frac{1}{2}\sum_{i=3}^N \epsilon_i(\theta)^2 \)


** The problem with least squares
*** Text
    :PROPERTIES:
    :BEAMER_col: 0.4
    :END:
   \begin{align*}
    \text{minimize} \; &\sum_k g(\epsilon_k)\\
    \text{where} \; g(u) &= u^2
   \end{align*}
   
*** Graphics
    :PROPERTIES:
    :BEAMER_col: 0.6
    :END:

    #+begin_export latex
        \begin{center}
          \begin{tikzpicture}
            \begin{axis}[
              width=8cm,
              height=6cm,
              ylabel=loss,
              xlabel=penalty,
              ]
              \addplot[red, thick, no marks, domain=-4:4, samples=201] {x^2};
            \end{axis}
          \end{tikzpicture}
        \end{center}

    #+end_export

** More robust: The Huber loss function
*** Text
    :PROPERTIES:
    :BEAMER_col: 0.4
    :END:
    Also known as *robust regression*
   \begin{align*}
    \text{minimize} \; &\sum_k g_{hub}(\epsilon_k)\\
    \text{where}\; g_{hub}(u) &= \begin{cases} u^2 & |u| \le M\\ M(2|u|-M) & |u| > M \end{cases}
   \end{align*}

*** Graphics
    :PROPERTIES:
    :BEAMER_col: 0.6
    :END:
    #+begin_export latex
        \begin{center}
          \begin{tikzpicture}
            \begin{axis}[
              width=8cm,
              height=6cm,
              ylabel=penalty,
              xlabel=residual,
              ]
              \addplot[red, thick, no marks, domain=-4:4, samples=201] {x^2};
              \addplot[orange!90!black, ultra thick, no marks, domain=-4:-1, samples=201] {2*abs(x)-1};
              \addplot[orange!90!black, thin, no marks, domain=-1:1, samples=201] {x^2};
              \addplot[orange!90!black, ultra thick, no marks, domain=1:4, samples=201] {2*abs(x)-1};
            \end{axis}
          \end{tikzpicture}
        \end{center}

    #+end_export


** First-order ARX model without delay

#+begin_center
\includegraphics[width=0.4\linewidth]{../../figures/block-arx}
#+end_center
 \[ (\shift + a_1) y(k) = (b_0 \shift + b_1) u(k) + e(k+1) \]

 *Activity*
 
1. Determine the one-step ahead predictor \(\hat{y}_{k+1}\) and the prediction error \(\epsilon_{k+1}\).
2. Form the system of equations \( \Phi\theta = y \) 


** The ARX model
\[ A(\shift) y(k) = B(\shift)u(k) + e(k+n)\]

*Activity* Fill the empty blocks.

#+begin_export latex
\begin{center}
  \begin{tikzpicture}[node distance=22mm, block/.style={rectangle, draw, minimum width=15mm, minimum height=12mm}, sumnode/.style={circle, draw, inner sep=2pt}]
    
    \node[coordinate] (input) {};
    \node[block, right of=input, node distance=20mm] (plant)  {};
    \node[sumnode, right of=plant, node distance=24mm] (sum) {\tiny $\Sigma$};
    \node[block, above of=sum, node distance=20mm] (dist)  {};

    \node[coordinate, above of=dist, node distance=12mm] (disturbance) {};
    \node[coordinate, right of=sum, node distance=20mm] (output) {};

    \draw[->] (input) -- node[above, pos=0.3] {$u(k)$} (plant);
    \draw[->] (plant) -- node[above] {} (sum);
    \draw[->] (sum) -- node[above, near end] {$y(k)$} (output);
    \draw[->] (disturbance) -- node[right, pos=0.2] {$e(k)$} (dist);
    \draw[->] (dist) -- node[above] {} (sum);

  \end{tikzpicture}
\end{center}

#+end_export


