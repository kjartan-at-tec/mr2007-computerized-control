#+OPTIONS: toc:nil
# #+LaTeX_CLASS: koma-article 

#+LATEX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [presentation,aspectratio=169]
#+OPTIONS: H:2

#+LaTex_HEADER: \usepackage{khpreamble}
#+LaTex_HEADER: \usepackage{amssymb}
#+LaTex_HEADER: \usepackage{pgfplotstable}
#+LaTex_HEADER: \usepackage{pgfplots}
#+LaTex_HEADER: \pgfplotsset{compat=1.16}
#+LaTex_HEADER: \DeclareMathOperator{\shift}{q}
#+LaTex_HEADER: \DeclareMathOperator{\diff}{p}

# #+LaTex_HEADER: \pgfplotstableread{./rbs_100.dta}\rbstable


#+title: System identification
# #+date: 2018-10-03

* What do I want the students to understand?			   :noexport:
  - Least squares parameter estimation

* Which activities will the students do?			   :noexport:
  1. Determine order of the controller
  2. Set up equations in controller parameters

* Intro
** A complicated process

*** Graphics 
   :PROPERTIES:
    :BEAMER_col: 0.4
    :END:
   #+begin_center
   \includegraphics[height=0.7\textheight]{../../figures/Vertical-cyclone.jpg}
   #+end_center

#+begin_export latex
\footnotesize
From Wikipedia "Cyclonic separation"
#+end_export

*** Graphics 
   :PROPERTIES:
    :BEAMER_col: 0.6
    :END:

   #+BEAMER: \pause
   
   \begin{center}
   \begin{tikzpicture}

   \begin{axis}[%
   width = 9cm,
   height = 5cm,
   xlabel = time,
   ]
   \addplot+[thick,cyan!90!black, no marks, domain = -2:11, samples=200] {2*(x>0.5)*(1-exp(-(x-0.5)/2))};
   \addplot+[green!80!black, no marks] coordinates {(-2, 0) (0,0) (0,1) (10,1)};
  \end{axis}
   \end{tikzpicture}
   \end{center}

   Fit model \[G(s) = \frac{k\mathrm{e}^{-s\theta}}{\tau s + 1} \]

** System identification
   #+begin_export latex
   \begin{center}
     \begin{tikzpicture}[node distance=22mm, block/.style={rectangle, draw, minimum width=15mm, inner sep=10pt}, sumnode/.style={circle, draw, inner sep=2pt},]
    
       \node[coordinate] (input) {};
       \node[coordinate, right of=input] (copy) {};
       \node[coordinate, right of=copy] (midp) {};
       \node[block, above of=midp, node distance=10mm] (sys)  {System};
       \node[block, below of=midp, node distance=10mm] (mod)  {Model};
       \node[sumnode, right of=midp, node distance=26mm] (sum) {\tiny $\Sigma$};
       \node[coordinate, right of=sum, node distance=22mm] (output) {};

       \draw[-] (input) -- node[above, pos=0.2] {Measured input} (copy);
       \draw[->] (copy) |- node[above] {} (sys);
       \draw[->] (copy) |- node[above] {} (mod);
       \draw[->] (sys) -| node[left, pos=0.9] {$+$} (sum);
       \draw[->] (mod) -| node[left, pos=0.9] {$-$} (sum);
       \draw[->] (sum) -- node[above, near end] {Error} (output);

       \draw[thick, red!70!black, ->] (2.7,-2) -- (3.3,-2) -- (5.3, 0);
     \end{tikzpicture}
   \end{center}

   #+end_export

** The Auto-Regressive with eXogenous input (ARX) model 

#+begin_center
\includegraphics[width=0.6\linewidth]{../../figures/block-arx}
#+end_center

#+beamer: \pause

\[ A(\shift) y(k) = B(\shift)u(k) + e(k+n) \]

    The error signal \(e(k)\) is a zero-mean white noise sequence representing perturbations and modeling errors.

** White noise

#+begin_export latex
\begin{center}
\begin{tikzpicture}
  \begin{axis}[axis line style={black!20},
  width=14cm,
  height=3.5cm,
  %xlabel={$t$},
  ylabel={$e(k)$},
  %xtick = {-1, 0, 1},
  %xticklabels={$k-1$, $k$, $k+1$},
  %xtick = {0, 2, 4, 6, 8, 10},
  %ytick=\empty,
  xmin=-.5,
  xmax=100.5,
]
  %\addplot[black, ycomb] table[x expr=\coordindex, y index=0] {\randntable};
  \addplot+[black, ycomb, domain=0:100, samples=101,variable=k] { rand}; 
\end{axis}
\end{tikzpicture}
\end{center}
#+end_export

** Random binary signal

#+begin_export latex
\begin{center}
\begin{tikzpicture}
  \begin{axis}[axis line style={black!20},
  width=14cm,
  height=3.5cm,
  %xlabel={$t$},
  ylabel={$u(k)$},
  %xtick = {-1, 0, 1},
  %xticklabels={$k-1$, $k$, $k+1$},
  %xtick = {0, 2, 4, 6, 8, 10},
  %ytick=\empty,
  %xmin=-.5,
  %xmax=100.5,
]
\addplot+[black, ycomb] table[x expr=\coordindex, y index=0] {../../figures/rbs_100.dta};

\end{axis}
\end{tikzpicture}
\end{center}
#+end_export

** First-order ARX model with one delay

#+begin_center
\includegraphics[width=0.3\linewidth]{../../figures/block-arx}
#+end_center
 \[ (\shift + \textcolor{red!60!black!}{a_1}) y(k) = (\textcolor{red!60!black!}{b_0} \shift + \textcolor{red!60!black!}{b_1}) \shift^{-1}u(k) + e(k+1) \]
 \[ y(k+1) +  \textcolor{red!60!black!}{a_1}y(k) = \textcolor{red!60!black!}{b_0}u(k) + \textcolor{red!60!black!}{b_1}u(k-1) + e(k+1) \]

 #+BEAMER: \pause
 
Using the model to predict the output one step ahead:
\begin{align*}
 \hat{y}(k+1) &= -\textcolor{red!60!black!}{a_1}y(k) + \textcolor{red!60!black!}{b_0}u(k) + \textcolor{red!60!black!}{b_1}u(k-1) =  \underbrace{\begin{bmatrix} \textcolor{white}{-y(k)} & \textcolor{white}{u(k)} & \textcolor{white}{u(k-1)} \end{bmatrix}}_{\varphi_{k+1}^T} \underbrace{\begin{bmatrix} \textcolor{red!60!black}{a_1}\\\textcolor{red!60!black}{b_0}\\\textcolor{red!60!black}{b_1}\end{bmatrix}}_{\theta}\\
 &= \varphi_{k+1}^T\textcolor{red!60!black}{\theta}
 \end{align*}

 


 
** First-order ARX model with one delay

#+begin_center
\includegraphics[width=0.3\linewidth]{../../figures/block-arx}
#+end_center
 \[ (\shift + \textcolor{red!60!black!}{a_1}) y(k) = (\textcolor{red!60!black!}{b_0} \shift + \textcolor{red!60!black!}{b_1}) \shift^{-1}u(k) + e(k+1) \]
 \[ y(k+1) +  \textcolor{red!60!black!}{a_1}y(k) = \textcolor{red!60!black!}{b_0}u(k) + \textcolor{red!60!black!}{b_1}u(k-1) + e(k+1) \]

 
Using the model to predict the output one step ahead:
\begin{align*}
 \hat{y}(k+1) &= -\textcolor{red!60!black!}{a_1}y(k) + \textcolor{red!60!black!}{b_0}u(k) + \textcolor{red!60!black!}{b_1}u(k-1) =  \underbrace{\begin{bmatrix} \textcolor{black}{-y(k)} & \textcolor{black}{u(k)} & \textcolor{black}{u(k-1)} \end{bmatrix}}_{\varphi_{k+1}^T} \underbrace{\begin{bmatrix} \textcolor{red!60!black}{a_1}\\\textcolor{red!60!black}{b_0}\\\textcolor{red!60!black}{b_1}\end{bmatrix}}_{\theta}\\
 &= \varphi_{k+1}^T\textcolor{red!60!black}{\theta}
 \end{align*}

 


** Parameter estimation - Least squares

*Objective* Given observations \[\mathcal{D} = \{ (u_1,y_1), (u_2, y_2), \ldots, (u_N, y_N)\}\] and model \( \mathcal{M}: \; y(k+1) = -\textcolor{red!60!black!}{a}y(k) + \textcolor{red!60!black!}{b_0}u(k) + \textcolor{red!60!black!}{b_1}u(k-1)  + e(k+1)\), obtain the parameters \( (\textcolor{red!60!black!}{a},\,\textcolor{red!60!black!}{b_0},\,\textcolor{red!60!black!}{b_1})\) which gives the best fit of the model to the data.

 

** Parameter estimation - Least squares
Given observations \[\mathcal{D} = \{ (u_1,y_1), (u_2, y_2), \ldots, (u_N, y_N)\}\] and model \( \mathcal{M}: \; y(k+1) = -\textcolor{red!60!black!}{a}y(k) + \textcolor{red!60!black!}{b_0}u(k) + \textcolor{red!60!black!}{b_1}u(k-1)  + e(k+1)\).

1. Form the one-step ahead prediction
 \[ \hat{y}_{k+1} = -\textcolor{red!60!black!}{a_1}y_k + \textcolor{red!60!black!}{b_0}u_k + \textcolor{red!60!black!}{b_1}u_{k-1} =  \underbrace{\begin{bmatrix} -y_k & u_k & u_{k-1} \end{bmatrix}}_{\varphi_{k+1}^T} \underbrace{\begin{bmatrix} \textcolor{red!60!black!}{a_1}\\\textcolor{red!60!black!}{b_0}\\\textcolor{red!60!black!}{b_1}\end{bmatrix}}_{\textcolor{red!60!black!}{\theta}}\] and the prediction error
    \[ \epsilon_{k+1} = y_{k+1} - \hat{y}_{k+1} = y_{k+1} - \varphi_{k+1}^T\textcolor{red!60!black!}{\theta}.\]


** Parameter estimation - Least squares

2. [@2] Combine all the observations \(y_k\) and predictions \(\hat{y}_k\) on vector form
   \begin{align*}
   \epsilon &= \begin{bmatrix} \epsilon_3\\\epsilon_4\\\vdots\\\epsilon_N\end{bmatrix} =  \begin{bmatrix} y_3\\ y_4\\\vdots\\y_N \end{bmatrix} - \begin{bmatrix} \hat{y}_3\\ \hat{y}_4\\\vdots\\\hat{y}_N \end{bmatrix}
    =  \begin{bmatrix} y_3\\ y_4\\\vdots\\y_N \end{bmatrix} - \begin{bmatrix} \varphi_3^T\textcolor{red!60!black!}{\theta}\\ \varphi_4^T\textcolor{red!60!black!}{\theta}\\\vdots\\\varphi_N^T\textcolor{red!60!black!}{\theta} \end{bmatrix}\\
   &= y - \underbrace{\begin{bmatrix}\varphi_3^T\\\varphi_4^T\\\vdots\\\varphi_N^T\end{bmatrix}}_{\Phi}\textcolor{red!60!black!}{\theta} = y - \Phi\textcolor{red!60!black!}{\theta} 
   \end{align*}
3. Solve \(\arg\min \; J(\textcolor{red!60!black!}{\theta}) = \frac{1}{2}\epsilon^T\epsilon = \frac{1}{2}\sum_{i=3}^N \epsilon_i(\textcolor{red!60!black!}{\theta})^2 \)


** The problem with least squares
*** Text
    :PROPERTIES:
    :BEAMER_col: 0.4
    :END:

    #+begin_export latex
        \begin{center}
          \begin{tikzpicture}
            \begin{axis}[
              width=6cm,
              height=5cm,
              %ylabel=loss,
              %xlabel=penalty,
	      xtick = \empty,
	      ytick = \empty,
              ]
              \addplot[black, no marks , domain=-2:2, samples=40] {x^3};
              \addplot[blue, only marks, domain=-2:2, samples=10] {x^3 + 2.3*rand};
            \end{axis}
          \end{tikzpicture}
        \end{center}

    #+end_export


   \begin{align*}
    \text{minimize} \; &\sum_k g(\epsilon_k)\\
    \text{where} \; g(u) &= u^2
   \end{align*}
   
*** Graphics
    :PROPERTIES:
    :BEAMER_col: 0.6
    :END:

    #+begin_export latex
        \begin{center}
          \begin{tikzpicture}
            \begin{axis}[
              width=8cm,
              height=6cm,
              ylabel=loss,
              xlabel=penalty,
              ]
              \addplot[red, thick, no marks, domain=-4:4, samples=201] {x^2};
            \end{axis}
          \end{tikzpicture}
        \end{center}

    #+end_export

** More robust: The Huber loss function
*** Text
    :PROPERTIES:
    :BEAMER_col: 0.4
    :END:
    Also known as *robust regression*
   \begin{align*}
    \text{minimize} \; &\sum_k g_{hub}(\epsilon_k)\\
    \text{where}\; g_{hub}(u) &= \begin{cases} u^2 & |u| \le M\\ M(2|u|-M) & |u| > M \end{cases}
   \end{align*}

*** Graphics
    :PROPERTIES:
    :BEAMER_col: 0.6
    :END:
    #+begin_export latex
        \begin{center}
          \begin{tikzpicture}
            \begin{axis}[
              width=8cm,
              height=6cm,
              ylabel=penalty,
              xlabel=residual,
              ]
              \addplot[red, thick, no marks, domain=-4:4, samples=201] {x^2};
              \addplot[orange!90!black, ultra thick, no marks, domain=-4:-1, samples=201] {2*abs(x)-1};
              \addplot[orange!90!black, thin, no marks, domain=-1:1, samples=201] {x^2};
              \addplot[orange!90!black, ultra thick, no marks, domain=1:4, samples=201] {2*abs(x)-1};
            \end{axis}
          \end{tikzpicture}
        \end{center}

    #+end_export


** First-order ARX model without delay

#+begin_center
\includegraphics[width=0.4\linewidth]{../../figures/block-arx}
#+end_center
 \[ (\shift + \textcolor{red!60!black!}{a_1}) y(k) = (\textcolor{red!60!black!}{b_0} \shift + \textcolor{red!60!black!}{b_1}) u(k) + e(k+1) \]

 *Activity*
 
1. Determine the one-step ahead predictor \(\hat{y}_{k+1}\) and the prediction error \(\epsilon_{k+1}\).
2. Form the system of equations \( \Phi\textcolor{red!60!black!}{\theta} = y \) 


** The ARMAX model
\[ A(\shift) y(k) = B(\shift)u(k) + C(\shift)e(k)\]

*Activity* Fill the empty blocks.

#+begin_export latex
\begin{center}
  \begin{tikzpicture}[node distance=22mm, block/.style={rectangle, draw, minimum width=15mm, minimum height=12mm}, sumnode/.style={circle, draw, inner sep=2pt}]
    
    \node[coordinate] (input) {};
    \node[block, right of=input, node distance=20mm] (plant)  {};
    \node[sumnode, right of=plant, node distance=24mm] (sum) {\tiny $\Sigma$};
    \node[block, above of=sum, node distance=20mm] (dist)  {};

    \node[coordinate, above of=dist, node distance=12mm] (disturbance) {};
    \node[coordinate, right of=sum, node distance=20mm] (output) {};

    \draw[->] (input) -- node[above, pos=0.3] {$u(k)$} (plant);
    \draw[->] (plant) -- node[above] {} (sum);
    \draw[->] (sum) -- node[above, near end] {$y(k)$} (output);
    \draw[->] (disturbance) -- node[right, pos=0.2] {$e(k)$} (dist);
    \draw[->] (dist) -- node[above] {} (sum);

  \end{tikzpicture}
\end{center}

#+end_export


