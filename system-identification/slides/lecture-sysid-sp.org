#+OPTIONS: toc:nil
# #+LaTeX_CLASS: koma-article 

#+LATEX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [presentation,aspectratio=169]
#+OPTIONS: H:2

#+LaTex_HEADER: \usepackage{khpreamble}
#+LaTex_HEADER: \usepackage{amssymb}
#+LaTex_HEADER: \usepackage{pgfplotstable}
#+LaTex_HEADER: \DeclareMathOperator{\shift}{q}
#+LaTex_HEADER: \DeclareMathOperator{\diff}{p}

#+title: Control computarizado - Identificación de sistemas
# #+date: 2018-10-03

* What do I want the students to understand?			   :noexport:
  - Least squares parameter estimation

* Which activities will the students do?			   :noexport:
  1. Determine order of the controller
  2. Set up equations in controller parameters

* Intro
** Identificación de sistemas

** Un proceso complejo

*** Graphics 
   :PROPERTIES:
    :BEAMER_col: 0.6
    :END:
 From Wikipedia "Cyclonic separation"
*** Graphics 
   :PROPERTIES:
    :BEAMER_col: 0.4
    :END:
   #+begin_center
   \includegraphics[height=1.0\textheight]{../../figures/Vertical-cyclone.jpg}
   #+end_center

** Identificación de sistemas
   #+begin_export latex
   \begin{center}
     \begin{tikzpicture}[node distance=22mm, block/.style={rectangle, draw, minimum width=15mm, inner sep=10pt}, sumnode/.style={circle, draw, inner sep=2pt},]
    
       \node[coordinate] (input) {};
       \node[coordinate, right of=input] (copy) {};
       \node[coordinate, right of=copy] (midp) {};
       \node[block, above of=midp, node distance=10mm] (sys)  {Sistema};
       \node[block, below of=midp, node distance=10mm] (mod)  {Modelo};
       \node[sumnode, right of=midp, node distance=26mm] (sum) {\tiny $\Sigma$};
       \node[coordinate, right of=sum, node distance=22mm] (output) {};

       \draw[-] (input) -- node[above, pos=0.2] {Señal medida} (copy);
       \draw[->] (copy) |- node[above] {} (sys);
       \draw[->] (copy) |- node[above] {} (mod);
       \draw[->] (sys) -| node[left, pos=0.9] {$+$} (sum);
       \draw[->] (mod) -| node[left, pos=0.9] {$-$} (sum);
       \draw[->] (sum) -- node[above, near end] {Error} (output);

       \draw[thick, red!70!black, ->] (2.7,-2) -- (3.3,-2) -- (5.3, 0);
     \end{tikzpicture}
   \end{center}

   #+end_export

** Ajustando un modelo - regresión lineal

*** Text
    :PROPERTIES:
    :BEAMER_col: 0.4
    :END:

    *Objetivo* Dado observaciones \[\mathcal{D} = \{ (x_1,y_1), (x_2, y_2), \ldots, (x_N, y_N)\}\] y 
    modelo \( \mathcal{M}: \; y = ax + b  + e\), obtiene los parametros \( (a,b)\) que da el modelo que mejor se ajuste a los datos.

    El término de ruido, o error, \(e\), incluye errores de modelación y perturbaciones.
*** Graphics
    :PROPERTIES:
    :BEAMER_col: 0.6
    :END:
   #+begin_center
   \includegraphics[height=0.6\textheight]{lsq-example}
   #+end_center


** Ajustando un modelo - regresión lineal

    Dado observaciones \(\mathcal{D} = \{ (x_1,y_1), (x_2, y_2), \ldots, (x_N, y_N)\}\) y 
    modelo \( \mathcal{M}: \; y = ax + b  + e\). 

*** Text
    :PROPERTIES:
    :BEAMER_col: 0.7
    :END:

    La predicción es
    \[ \hat{y_k} = ax_k + b = \underbrace{\begin{bmatrix} x & 1 \end{bmatrix}}_{\varphi_k^T} \underbrace{\begin{bmatrix} a\\b\end{bmatrix}}_{\theta}\]
    y el error de predicción 
    \[ \epsilon_k = y_k - \hat{y}_k = y_k - ax_k-b = y - \varphi_k^T\theta.\]

    Buscamos parametros \(\theta^T = \begin{bmatrix} a & b \end{bmatrix}\) que minimiza
     la función de pérdida \[J(\theta) =  \sum_{k=1}^N g\big(\epsilon_k\big).\]
    
*** Graphics
    :PROPERTIES:
    :BEAMER_col: 0.3
    :END:
   #+begin_center
   \includegraphics[height=0.4\textheight]{lsq-example}
   #+end_center


** Ajustando un modelo - regresión lineal

    Dado observaciones \(\mathcal{D} = \{ (x_1,y_1), (x_2, y_2), \ldots, (x_N, y_N)\}\) y 
    modelo \( \mathcal{M}: \; y = ax + b  + e\). 

*** Text
    :PROPERTIES:
    :BEAMER_col: 0.6
    :END:
    
    La función de pérdida más común es *mínimos cuadrados*

    \begin{align*}
    \hat{\theta}_{LS} &= \arg\min J_{LS}(\theta) = \arg\min \sum_{k=1}^N \epsilon_k^2\\
    &= \arg\min \sum_{k=1}^N (y_k - \hat{y}_k)^2 
    = \arg\min \sum_{k=1}^N (y_k - \varphi_k\T\theta)^2\\ 
    &= \arg\min \sum_{k=1}^N (y_k - ax_k - b)^2
    \end{align*}
    
*** Graphics
    :PROPERTIES:
    :BEAMER_col: 0.4
    :END:
   #+begin_center
   \includegraphics[height=0.5\textheight]{lsq-example}
   #+end_center



** El problema con mínimos cuadrados
*** Text
    :PROPERTIES:
    :BEAMER_col: 0.4
    :END:
   \begin{align*}
    \text{minimiza} \; &\sum_k g(\epsilon_k)\\
    \text{dónde} \; g(u) &= u^2
   \end{align*}
   
*** Graphics
    :PROPERTIES:
    :BEAMER_col: 0.6
    :END:

    #+begin_export latex
        \begin{center}
          \begin{tikzpicture}
            \begin{axis}[
              width=8cm,
              height=6cm,
              ylabel=pérdida,
              xlabel=residual,
              ]
              \addplot[red, thick, no marks, domain=-4:4, samples=201] {x^2};
            \end{axis}
          \end{tikzpicture}
        \end{center}

    #+end_export

** Más robusta: La función de pérdida de Huber
*** Text
    :PROPERTIES:
    :BEAMER_col: 0.4
    :END:
    También conocido como *regresión robusta*
   \begin{align*}
    \text{minimiza} \; &\sum_k g_{hub}(\epsilon_k)\\
    \text{dónde}\; g_{hub}(u) &= \begin{cases} u^2 & |u| \le M\\ M(2|u|-M) & |u| > M \end{cases}
   \end{align*}

*** Graphics
    :PROPERTIES:
    :BEAMER_col: 0.6
    :END:
    #+begin_export latex
        \begin{center}
          \begin{tikzpicture}
            \begin{axis}[
              width=8cm,
              height=6cm,
              ylabel=penalty,
              xlabel=residual,
              ]
              \addplot[red, thick, no marks, domain=-4:4, samples=201] {x^2};
              \addplot[orange!90!black, ultra thick, no marks, domain=-4:-1, samples=201] {2*abs(x)-1};
              \addplot[orange!90!black, thin, no marks, domain=-1:1, samples=201] {x^2};
              \addplot[orange!90!black, ultra thick, no marks, domain=1:4, samples=201] {2*abs(x)-1};
            \end{axis}
          \end{tikzpicture}
        \end{center}

    #+end_export

* AR-model

** Ejemplo - Modelo autoregresivo (AR)
** Modelo autoregresivo (AR)
Dado una secuencia discreta observada \(y(k), \; k=1,2,\ldots,N\), y el modelo autoregresivo
\[ y(k+1) = -ay(k) + e(k+1),\]
dónde \(e(k)\) es una sequencia discreta de ruido blanco.

*Objetivo* Estimar el parametro \(a\).

1. Forma el predictor de un paso adelante \[\hat{y}_{k+1} = -ay_k=-y_ka = \varphi_{k+1} \theta,\] y el error de predicción \[\epsilon_k = y_k - \hat{y}_k = y_k - \varphi_k \theta\]


** Modelo autoregresivo (AR)
Dado una secuencia discreta observada \(y(k), \; k=1,2,\ldots,N\), y el modelo autoregresivo
\( y(k+1) = -ay(k) + e(k+1),\)
dónde \(e(k)\) es una sequencia discreta de ruido blanco.

*Objetivo* Estimar el parametro \(a\).

2. [@2] Reune todas las observaciónes \(y_k\) y predicciones \(\hat{y}_k\) en forma vectoral
   \begin{align*}
   \epsilon &= \begin{bmatrix} \epsilon_2\\\epsilon_2\\\vdots\\\epsilon_N\end{bmatrix} =  \begin{bmatrix} y_2\\ y_3\\\vdots\\y_N \end{bmatrix} - \begin{bmatrix} \hat{y}_2\\ \hat{y}_3\\\vdots\\\hat{y}_N \end{bmatrix}
    =  \begin{bmatrix} y_2\\ y_3\\\vdots\\y_N \end{bmatrix} - \begin{bmatrix} -y_1 a\\ -y_2 a\\\vdots\\-y_{N-1}^T\theta \end{bmatrix} =  \begin{bmatrix} y_2\\ y_3\\\vdots\\y_N \end{bmatrix} - \begin{bmatrix} \varphi_2^T\theta\\ \varphi_3^T\theta\\\vdots\\\varphi_N^T\theta \end{bmatrix}\\
   &= y - \underbrace{\begin{bmatrix}\varphi_1^T\\\varphi_2^T\\\vdots\\\varphi_N^T\end{bmatrix}}_{\Phi}\theta = y - \Phi\theta 
   \end{align*}



** Modelo autoregresivo (AR)
Dado una secuencia discreta observada \(y(k), \; k=1,2,\ldots,N\), y el modelo autoregresivo
\( y(k+1) = -ay(k) + e(k+1),\)
dónde \(e(k)\) es una sequencia discreta de ruido blanco.

*Objetivo* Estimar el parametro \(a\).

3. [@3] Obtiene el estimado de mínimos cuadrados 
   \begin{align*}
    \theta_{LS} &= (\Phi^T\Phi)^{-1}\Phi^T y\\ &= \left(\begin{bmatrix} -y_1 & -y_2 & \cdots & -y_{N-1}\end{bmatrix}\begin{bmatrix}-y_1\\-y_2\\\vdots\\-y_{N-1}\end{bmatrix}\right)^{-1}\begin{bmatrix} -y_1 & -y_2 & \cdots & -y_{N-1}\end{bmatrix}\begin{bmatrix}y_2\\y_3\\\vdots\\y_N\end{bmatrix}\\
    &= -\frac{\sum_{k=1}^{N-1} y_ky_{k+1}}{\sum_{k=1}^{N-1}y_k^2}
    \end{align*}


** Computación de la solución de mínimos cuadrados
   Dado error de predicción en forma vectoral para sistema de orden $n$
   \( \epsilon = y - \Phi\theta\). Forma el sistema de ecuaciones
   \begin{align*}
   \Phi \theta &= y\\
   \begin{bmatrix}\varphi_{n+1}^T\\\varphi_{n+2}^T\\\varphi_{n+3}^T\\\varphi_{n+4}^T\\\vdots\\\varphi_{N}^T\end{bmatrix} \begin{bmatrix}\theta_1\\\theta_2\\\vdots\\\theta_m\end{bmatrix} &= \begin{bmatrix}y_{n+1}\\y_{n+2}\\y{n+3}\\y_{n+4}\\\vdots\\ y_{N}\end{bmatrix}
   \end{align*}
   Resuelva las ecuaciones usando métodos numericamente robustos de algebra lineal, por ejemplo   factorización L-U. En matlab se escribe
   #+begin_src octave
	theta_LS = Phi \ y
   #+end_src
   
** Ejemplo numerico 
  
   [[https://mybinder.org/v2/gh/kjartan-at-tec/mr2007-computerized-control/master?filepath=.%2Fsystem-identification%2Fnotebooks%2FAR-example.ipynb][Mybinder]]



** Model AR de orden \(n\)   
Dado una secuencia discreta observada \(y(k), \; k=1,2,\ldots,N\), y el modelo autoregresivo
\begin{align*} 
A(z)Y(z) = z^nE(z) \quad &\Leftrightarrow \quad A(\shift)y(k) = \shift^{n-1} e(k)\\
(\shift^n + a_1\shift^{n-1} + a_2\shift^{n-2} + \cdots + a_n)y(k) &= \shift^n e(k)\\
(\shift + a_1 + a_2\shift{-1} + \cdots + a_n\shift^{-n+1})y(k) &= \shift^{n-1}e(k)\\
y(k+1) + a_1y(k)  + a_2y(k-1) + \cdots + a_ny(k-n+1) &= e(k+1)\\
y(k+1) = -a_1y(k)  - a_2y(k-1) - \cdots - a_ny(k-n+1) &+ e(k+1)
\end{align*}
dónde \(e(k)\) es una sequencia discreta de ruido blanco.

*Objetivo* Estimar los parametro \(a_1, a_2, \ldots, \a_n\).


** Model AR de orden \(n\)   
Dado una secuencia discreta observada \(y(k), \; k=1,2,\ldots,N\), y el modelo autoregresivo
\(y(k+1) = -a_1y(k)  - a_2y(k-1) - \cdots - a_ny(k-n+1) + e(k+1)\).

1. Forma el predictor de un paso adelante 
   \[\hat{y}_{k+1} = -a_1y_k-a_2y_{k-1} - \ldots - a_n y_{k-n+1}a = \underbrace{\begin{bmatrix} -y_{k} & -y_{k-1} & \cdots & -y_{k-n+1}\end{bmatrix}}_{\varphi_{k+1}^T}\underbrace{\begin{bmatrix}a_1\\a_2\\\vdots\\a_n\end{bmatrix}}_{\theta}\]
   y el error de predicción \[\epsilon_k = y_k - \hat{y}_k = y_k - \varphi_k^T \theta\]

** Model AR de orden \(n\)   
Dado una secuencia discreta observada \(y(k), \; k=1,2,\ldots,N\), y el modelo autoregresivo
\(y(k+1) = -a_1y(k)  - a_2y(k-1) - \cdots - a_ny(k-n+1) + e(k+1)\).

2. [@2] Reune todas las observaciónes \(y_k\) y predicciones \(\hat{y}_k\) en forma vectoral
   \begin{align*}
   \epsilon &= \begin{bmatrix} \epsilon_{n+1}\\\epsilon_{n+2}\\\vdots\\\epsilon_N\end{bmatrix} =  \begin{bmatrix} y_{n+1}\\ y_{n+2}\\\vdots\\y_N \end{bmatrix} - \begin{bmatrix} \hat{y}_{n+1}\\ \hat{y}_{n+2}\\\vdots\\\hat{y}_N \end{bmatrix}
    =  \begin{bmatrix} y_{n+1}\\ y_{n+2}\\\vdots\\y_N \end{bmatrix} - \begin{bmatrix} \varphi_{n+1}^T\theta\\ \varphi_{n+2}^T\theta\\\vdots\\\varphi_N^T\theta \end{bmatrix}\\
   &= y - \underbrace{\begin{bmatrix}\varphi_{n+1}^T\\\varphi_{n+2}^T\\\vdots\\\varphi_N^T\end{bmatrix}}_{\Phi}\theta = y - \Phi\theta 
   \end{align*}

** Model AR de orden \(n\)   
Dado una secuencia discreta observada \(y(k), \; k=1,2,\ldots,N\), y el modelo autoregresivo
\(y(k+1) = -a_1y(k)  - a_2y(k-1) - \cdots - a_ny(k-n+1) + e(k+1)\).
3. [@3] Obtiene el estimado de mínimos cuadrados, que es
   \begin{align*}
    \theta_{LS} &= (\Phi^T\Phi)^{-1}\Phi^T y
    \end{align*}
   formando y resolviendo el sistema de ecuaciones
   \begin{align*}
   \Phi \theta &= y\\
   \begin{bmatrix}\varphi_{n+1}^T\\\varphi_{n+2}^T\\\varphi_{n+3}^T\\\varphi_{n+4}^T\\\vdots\\\varphi_{N}^T\end{bmatrix} \begin{bmatrix}a_1\\a_2\\\vdots\\a_n\end{bmatrix} &= \begin{bmatrix}y_{n+1}\\y_{n+2}\\y_{n+3}\\y_{n+4}\\\vdots\\ y_{N}\end{bmatrix}
   \end{align*}


** Modelo autoregresivo (AR) - Ejercicio
Dado una secuencia discreta observada \(y(k), \; k=1,2,\ldots,N\), y el modelo autoregresivo de segunda orden
\[ y(k+2) + a_1y(k+1) + a_2y(k) = e(k+2),\]
dónde \(e(k)\) es una sequencia discreta de ruido blanco.

*Actividad* Forma las ecuaciones \[ \Phi \theta = y\]


* ARX-model

** Model AutoRegresivo con variables eXógenas (ARX)  
Dado señal discreta de entrada de un sistema \(u(k), \; k=1,2,\ldots, N\) y observaciones de la respuesta \(y(k), \; k=1,2,\ldots,N\), y el modelo ARX
\[ A(\shift) y(k) = B(\shift)u(k) + e(k+n),\]
dónde \(e(k)\) es una sequencia discreta de ruido blanco.

*Actividad* Llena los bloques

#+begin_export latex
\begin{center}
  \begin{tikzpicture}[node distance=22mm, block/.style={rectangle, draw, minimum width=15mm, minimum height=12mm}, sumnode/.style={circle, draw, inner sep=2pt}]
    
    \node[coordinate] (input) {};
    \node[block, right of=input, node distance=20mm] (plant)  {};
    \node[sumnode, right of=plant, node distance=24mm] (sum) {\tiny $\Sigma$};
    \node[block, above of=sum, node distance=20mm] (dist)  {};

    \node[coordinate, above of=dist, node distance=12mm] (disturbance) {};
    \node[coordinate, right of=sum, node distance=20mm] (output) {};

    \draw[->] (input) -- node[above, pos=0.3] {$u(k)$} (plant);
    \draw[->] (plant) -- node[above] {} (sum);
    \draw[->] (sum) -- node[above, near end] {$y(k)$} (output);
    \draw[->] (disturbance) -- node[right, pos=0.2] {$e(k)$} (dist);
    \draw[->] (dist) -- node[above] {} (sum);

  \end{tikzpicture}
\end{center}

#+end_export


** Model AutoRegresivo con variables eXógenas (ARX) - solución
Dado señal discreta de entrada de un sistema \(u(k), \; k=1,2,\ldots, N\) y observaciones de la respuesta \(y(k), \; k=1,2,\ldots,N\), y el modelo ARX
\[ A(\shift) y(k) = B(\shift)u(k) + e(k+n),\]
dónde \(e(k)\) es una sequencia discreta de ruido blanco.

*Actividad* Llena los bloques

#+begin_export latex
\begin{center}
  \begin{tikzpicture}[node distance=22mm, block/.style={rectangle, draw, minimum width=15mm, minimum height=12mm}, sumnode/.style={circle, draw, inner sep=2pt}]
    
    \node[coordinate] (input) {};
    \node[block, right of=input, node distance=20mm] (plant)  {$\frac{B(z)}{A(z)}$};
    \node[sumnode, right of=plant, node distance=24mm] (sum) {\tiny $\Sigma$};
    \node[block, above of=sum, node distance=20mm] (dist)  {$\frac{z^n}{A(z)}$};

    \node[coordinate, above of=dist, node distance=12mm] (disturbance) {};
    \node[coordinate, right of=sum, node distance=20mm] (output) {};

    \draw[->] (input) -- node[above, pos=0.3] {$u(k)$} (plant);
    \draw[->] (plant) -- node[above] {} (sum);
    \draw[->] (sum) -- node[above, near end] {$y(k)$} (output);
    \draw[->] (disturbance) -- node[right, pos=0.2] {$e(k)$} (dist);
    \draw[->] (dist) -- node[above] {} (sum);

  \end{tikzpicture}
\end{center}

#+end_export



** Model ARX de orden \(n\)   
Dado señal discreta de entrada de un sistema \(u(k), \; k=1,2,\ldots, N\) y observaciones de la respuesta \(y(k), \; k=1,2,\ldots,N\), y el modelo ARX \(A(\shift)y(k) = B(\shift)u(k-d) + \shift^n e(k)\) con \(n\) polos, \(m\) ceros y retraso de \(d\) pasos
\begin{align*} 
A(z)Y(z) = B(z)z^{-d}U(z) + z^nE(z) \quad &\Leftrightarrow \quad A(\shift)y(k) = B(\shift)\shift^{-d}u(k) + \shift^{n-1} e(k)\\
(\shift^n + a_1\shift^{n-1} + a_2\shift^{n-2} + \cdots + a_n)y(k) &= (b_0\shift^{m-d} + b_1\shift^{m-d-1} + \cdots + b_m\shift^{-d})u(k) +  \shift^n e(k)\\
(\shift + a_1 + a_2\shift^{-1} + \cdots + a_n\shift^{-n+1})y(k) &= (b_0\shift^{m-d-n+1} + b_1\shift^{m-d-n} + \cdots + b_m\shift^{-d-n+1})u(k) +  \shift e(k)\\
y(k+1) =  -a_1y(k) - \cdots - a_ny(k-n+1) + b_0u(k+m-n-d+1) + \cdots + b_mu(k-n-d+1) &=   e(k+1)
\end{align*}

*Objetivo* Estimar los parametro \(a_1, a_2, \ldots, \a_n, b_0, b_1, \ldots, b_m\).


** Ejemplo y tarea

   [[https://mybinder.org/v2/gh/kjartan-at-tec/mr2007-computerized-control/master?filepath=.system-identification%2Fnotebooks%2FParameter%20estimation%20with%20least%20squares%20-%20Homework.ipynb][Mybinder]]

